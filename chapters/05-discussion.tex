\chapter{Discussion}
\label{ch:discussion}

\section{Introduction}

The aim of the project was to design and build a prototype of a High-Altitude
Balloon modular platform, similar in principle to buses used in the satellite
industry, and evaluate both the feasibility of such a platform and its
performance. The platform was required to allow for stable internal
communications between the payloads and the \acrlong{obc}, tracking of
ancillary data (\acrshort{gnss} co-ordinates and altitude) and reliable
transmission of the collected telemetry to one or multiple ground stations 
through a low-power radio channel. In this chapter, the \acrshort{ahabus}
platform prototype is evaluated against these aims, first through general 
qualitative observations and analysis of the prototype's behaviour, then by
analysing the results of the tests discussed in chapter \ref{ch:results}.

\section{Project Evaluation}

Overall, the project could be considered a success. As further section discuss,
some issues were encountered, and most aspects of the \acrshort{ahabus}
platform, protocols and toolchain still require refinement and further testing,
but the results and observation presented in chapter \ref{ch:results} showed
that developing a modular platform for \acrlong{hab} missions based on the
concept of satellite buses is feasible.

\subsection{Platform}

As a software platform, the \acrshort{ahabus} prototype fulfils most of its
goals: the flight \acrshort{fcore} was complete enough to support full-flight
simulations, manage the bus, \acrshort{gnss} and radio communications without
failure. During the testing phase, no major software design error were found,
and the only instances where failures were found to be caused by the flight
software itself. Though quite cumbersome to work with, and lacking in
documentation, the \acrshort{fcore} configuration tool discussed in section
\ref{ssec:conf-file} proved reliable, and the resulting \acrshort{fcore} builds
did not suffer from erroneous bandwidth usage.

There are, however, areas that could be improved upon: the software design of
\acrshort{fcore} makes use of numerous data buffers for the different
transmission layers. Those buffers are required because of the slow speeds that
those communications protocol use, but are at the moment not optimised to be as
small as required – instead, all buffers can hold \SI{4096}{\byte}. This would
prevent the current source code to be ported to architectures that provide less
dynamic memory than the chosen ESP8266 micro-controller does.

While the platform proved reliable and would be well suited for scientific
missions, where most of the generated telemetry is numerical measurements, it
might be less adequate for student missions where most users would expect to
transmit more data, like real-time pictures: the packets size limit
(\SI{420}{\byte}) would make transmitting any reasonably-sized image impossible.
This issue could potentially be solved by adding time division algorithms other
than the one presented in section \ref{ssec:conf-file}.

Due to lack of time, very little work was done on the hardware aspect of
\acrshort{ahabus}: testing was done using a development board instead of the
bare micro-controller for the \acrlong{obc}, and the power and payload bus were
little more than wires routed through a breadboard. An improved development
platform using a printed circuit board was designed as presented in
\ref{sec:flight-hardware}, but could not be fully assembled and tested in time.

\subsection{Radio Communications System}

The radio protocol and its software implementation proved equally time consuming
and successful. For reasons explained in this section, the protocol could not
be considered final or perfect, but the version developed over the duration of
the project proved reliable enough to transmit large amounts of data over long
simulated test missions.

There were however some observed issues, both in the design of the protocol and
in the frame and packet decoder implementation. First, the current use of a
16-bit unsigned integer to encode the platform's altitude in packet's header
could be problematic even though it did not cause problems during testing: first
because some locations on Earth are located below the mean sea level, leading to
negative altitudes that cannot be encoded with unsigned integers. Then because
the use of 16 bits, while sufficient to encode the altitudes at which balloons
normally fly, leave little to no margin for expansion, for example to
\acrshort{hab}-launched small rockets, or eventually nano-satellites.

Furthermore, it was observed that the reliance on frame length to detect the
end of a frame is error-prone. The current implementation of the frame decoder
counts the 256 bytes that follow a frame start marker as the frame,
unconditionally. As described in section \ref{ssec:results-rtx}, this causes
an issue where dropped bytes in the middle of the frame will cause the start
of the following frame to be consumed as the end of the current one.

One solution to this issue would be the use of an end-of-frame marker instead,
and to reject frames that are shorter than 256 bytes. However, this approach
would be challenging to implement in the context of the \acrshort{ahabus} radio
protocol. Because the end marker value might occur in the frame's content, it
must be escaped so that the decoder does not interpret a content byte as the
frame end. This can be achieved through \textit{byte stuffing}: an escape value
is chosen, and any end marker or escape value that occurs in the content must
be preceded by the escape value. The use of \acrlong{fec} in the protocol
further complicates the issue: the \acrlong{ecc} would have to be escaped too,
making frames impossible to error-correct before the escaped bytes have been
resolved by the decoder. This means that a single corrupted escape byte in a
received frame would prevent the frame from being corrected at all, even if the
number of invalid bytes is inferior to the \acrshort{fec} algorithm limit.

\subsection{Payload Bus}
\label{ssec:discussion-bus}

The payload data bus worked well overall, even though payloads failed to
respond in certain occurrences. Due to the absence of debugging facilities on
the Wemos D1 development board and lack of time, the root cause of this issue
could not be successfully identified. One possibility is the difference in
logical levels between the \acrlong{obc} (\SI{3.3}{\volt}) and the test
payloads (\SI{5.0}{\volt}). While in practice both side were able to detect
logical highs and lows, it is possible that performance could be improved by
using a voltage regulator on the data and clock lines. Additionally, the test
payloads were written using the Arduino libraries, which provide an easy-to-use
API to interact with AVR-based boards, but include a lot of overhead. It is
possible that some length interrupt service routines or I/O actions were
delaying the payloads' responses to \acrshort{i2c} requests, causing the
\acrshort{obc} to flag them as unresponsive.

Of all the systems designed for this project, the payload bus was the most
simple – it relies heavily on \acrshort{i2c}, an existing protocol – but also
the one that was the most complex to test thoroughly, and the one that would
require the most additional testing to be approved before a real test mission.

\subsection{Ground Segment}

The ground segment software was found to be reliable and sufficient for the
initial round of testing. All requirements were met: the modified version of
Dl-Fldigi was stable during the tests and successfully broadcasted the decoded
raw binary stream. The packet decoder/parser was sufficient to gather
information about the radio communication statistics and the platform's health
during missions, and received payload data was successfully written to disk
and marked as either valid or invalid.

While the current toolchain was sufficient for testing, and would likely be
usable for further tests, better packet forwarding would be required for a
finished software suite. While the current solution writes data directly to the
local drive of the receiving computer, a better solution might be to forward
decoded data and metadata for each packet to a socket on the local network, or
a server, so that client applications can be built for each team. This would
allow customised dashboards more apt to decode payload-specific data, and the
parser would only act as a dispatcher, and possibly a way to monitor the
\acrshort{ahabus} platform itself during its flight.

\section{Analysis of Test Results}

This section aims to explain and analyse the quantitative results obtained
during the testing phase of the project. The results are presented in chapter
\ref{ch:results}, and the raw data files obtained from each tests are made available in the repositories specified in appendix \ref{apx:test-data-repos}.

\subsection{Day-in-the-Life Test}

The main goal of the day-in-the-life test was to evaluate whether the platform
was viable for full-duration \acrlong{hab} missions. In that respect, the test
can be considered successful: as shown in figure \ref{fig:ditl-alt}, the
simulation spanned a whole mission sequence, from launch through burst altitude,
to descent and landing, over a slightly more than two hours and twenty minutes.

While actions were taken for the test to mirror an actual mission as much as
possible, like the use of fake \acrshort{gnss} messages, one effect in
particular that had to be ignored was the weakening of the radio signal: in a
real mission, the increase in the distance between the ground station and the
platform over time means that the radio channel qualities (signal-to-noise
ratio, line of sight interruptions) are not constant. For this test, neither the
transmission nor the reception antenna were moved. Range resilience would have
to be tested through dedicated tests before the platform is used on real
missions.

The test allowed a long duration check of the radio protocol and systems. Figure
\ref{fig:ditl-bandwidth} shows that the amount of corrupted frames was well
below the total amount of data that was received. The average rate of
transmission, however, was only slightly above half of the available bandwidth,
despite the use of the calculation of the optimal intervals using the tool
discussed in section \ref{ssec:conf-file}.

This disparity between actual and optimal use of the bandwidth was caused by the payloads: the time division formula used to compute the polling interval solves for the case where each payload always transmit exactly the maximum amount of data that can be contained in packet. During the test, one of the payload generated data at a low enough rate that it fit into small packets that could be encapsulated into a single frame, reducing the required bandwidth. This issue would be challenging to address with the current architecture since it requires the polling intervals to be fixed. A better use of the bandwidth could be approached by selecting appropriate payload priority levels during the mission design phase, or designing payload to use the full capacity of the radio communications layer.

It is important to note that the amount of erroneous data received was lower
than that reported by the decoder/parser, due to software design. 256 bytes are
added to the erroneous data count when a frame has too many errors to be
error-corrected, or when frames are skipped – the current frame sequence number
is higher than expected. When a corrupt frame is detected, the last decoded
sequence number is not changed to avoid overwriting it with a potentially
corrupted value. Because of this, the next valid frame to be detected will
also cause a dropped frame to be reported, even though the corrupt's frame
256 bytes have already been recorded.

Finally, table \ref{tab:ditl-health} shows that even though the data bus
performed well overall, both test payloads were marked as unresponsive in
nearly \SI{10}{\percent}, which can be considered high. As discussed in section
\ref{ssec:discussion-bus}, a single root cause could not be identified for this
particular issue, and more testing would be required.

\subsection{Radio System Resilience Tests}

The resilience tests were aimed at testing the reliability of the \acrlong{fec}
algorithm used in the radio protocol's frame layer. Those tests were considered
critical because the frame layer must provide reliability to the
\acrshort{ahabus} radio protocol. Since the chosen \acrshort{fec} algorithm,
Reed-Solomon codes $RS(255,223)$, can fix a maximum of 16 bytes per 256-byte
code block, the tests were designed to simulate average error rates below (8
bytes out of 256), near (14 bytes out of 256) and above (18 bytes out of 256
that theoretical limit, through the software method discussed in section
\ref{ssec:method-resilience}.

Overall, figures \ref{fig:stress-8.256}, \ref{fig:stress-14.256} and
\ref{fig:stress-18.256} show the expected behaviour: the higher the average
error rate, the more erroneous frames. It is interesting to note that even at
the two lower rates ($\frac{8}{256}$ and $\frac{16}{256}$), which are below
the maximum rate that the \acrlong{ecc} should be able to fix, some frames are
dropped. The main cause is the use of start sequences. While \acrshort{ecc} are
able to correct up to 16 erroneous bytes, the detection of the frame itself is a
pre-requisite. If the frame start marker itself is corrupted, the decoder cannot
identify the frame, and it will is counted as lost, even if the total number
of erroneous bytes in the frame is less than 16.

Additionally, it is important to note that the target error rates were averages.
For those three tests, the code used was set to corrupt the current byte if a
pseudo-random number between 0.0 and 1.0, obtained through the C standard
library's \texttt{rand()} function, was above the error rate. \texttt{rand()},
while allowing a good estimate, is not statistically accurate, and some frames
had a higher or lower than average erroneous bytes. This is believed to be the
source of the discrepancy between the theoretical average error rates and the
actual correction rates shown for the first two tests in table
\ref{tab:stress-fec}.

Finally, the third tests demonstrates the inaptitude of the \acrshort{ecc}
to cope when the average error rate is above the the algorithm's maximum. Most
of the frames were lost, the only few decoded being just under the threshold
due to the fluctuations discussed above. Table \ref{tab:stress-fec} shows that
the correction rate is significantly lower than the $\frac{14}{256}$ test, which
was expected: Reed-Solomon codes cannot fix even some errors if the number of
erroneous bytes in a code block is above the chosen algorithm's limit.

Overall, these three tests showed that the \acrshort{fec} algorithm chosen for
the frame layer worked as expected, so long as the error rate remains under
16 bytes per code block (frame). Ultimately, this limit is a trade-off between
reliability and bandwidth: raising it would require more bytes in each frame
to be allocated to \acrlong{ecc}.
